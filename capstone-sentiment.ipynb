{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "data = requests.get('https://covidtracking.com/api/states/daily').json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 136.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame.from_dict(data, orient='columns')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\pharm\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: PySocks>=1.5.7 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from tweepy) (1.7.1)\n",
      "Requirement already satisfied: requests>=2.11.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from tweepy) (2.23.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from tweepy) (1.14.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from requests>=2.11.1->tweepy) (1.25.8)\n",
      "RT @SpirosMargaris: How #AI Can Help \n",
      "\n",
      "Manage #MentalHealth In Times Of #Crisis \n",
      "\n",
      "https://t.co/6UVoX0TJqp #fintech #insurtech #healthcare #â€¦\n",
      "RT @DataZifu: IIT Roorkee launches online course on artificial intelligence, deep learning: Sign up and boost your skills in lockdown! #Datâ€¦\n",
      "Laser Scanner Security #Robots can contact police assistance, avoid obstacles and detect illegal parking\n",
      "byâ€¦ https://t.co/IzSUr4sksj\n",
      "Follow for all types of good info. https://t.co/No79qA6paQ\n",
      "@StyleSTEAMed @LearnVizWithMe @FlerlageKev @ZachBowders @tableauing @datavizlinds @ZenDollData @quantum_relicâ€¦ https://t.co/bx31PP1L2t\n",
      "What are the Top 10 Data Science and AI Books of 2020 https://t.co/maBxFJTDcd  #DataScience\n",
      "Machine learning is a sweet way to tell if your honey is fake \n",
      "https://t.co/lh9XUg7kSU\n",
      "RT @Paula_Piccard: When #Art meets #AR\n",
      "\n",
      "#IoT #AugmentedReality #AI #EmergingTech \n",
      "\n",
      "@CathyHackl @antgrasso @Hal_Good @RobCrasco \n",
      "@YvesMulkerâ€¦\n",
      "6 #EmergingTechnologies Finding Competitive Advantage In 2019\n",
      "by @PwC @MikeQuindazzi\n",
      "\n",
      "#Digital #Tech #Technologyâ€¦ https://t.co/jTSTqSSLEw\n",
      "Using #ArtificialIntelligence To Detect Product Defects With AWS Step Functions\n",
      "by @virgilvox @awscloud\n",
      "\n",
      "Read moreâ€¦ https://t.co/EFBqr9U5oV\n",
      "6 Machine Learning Concepts for Beginners https://t.co/oxCkrRny2l #MachineLearning\n",
      "Six Companies with Great Job Opportunities for Data Scientists https://t.co/Aan5Zjm3Cb\n",
      "Who want to win 100,000$ ?? ðŸ¤”ðŸ¤” https://t.co/0LQ5Vje4aR\n",
      ".@Talend's annual event, #TalendConnect, is near! Don't miss their opening keynote: 'Find Clarity Amidst the Chaosâ€¦ https://t.co/10ugueqgv8\n",
      "RT @IAM__Network: RT Via: https://t.co/iBhItze3aS\n",
      "\n",
      "[FREE 28-page PDF eBook] â†’ #Probability and #Statistics Cookbook for #DataScientists (anâ€¦\n",
      "RT @aliyu_01: Donâ€™t forget to bring  your basic linear algebra skills to ML/AI class.\n",
      "\n",
      " Matrices \n",
      "Vectors \n",
      "Eigenvalues \n",
      "Eigenvectors\n",
      "Dot &amp;â€¦\n",
      "RT @bymiachang: Next Saturday, stay tuned at @CsharpCorner \n",
      "#conversational #ai #Microsoft https://t.co/gnAieu2EWS\n",
      "RT @Ronald_vanLoon: How Deepfake #Technology Will Enable The Next #BigData Breach\n",
      "by @analyticsinme\n",
      "\n",
      "Go to https://t.co/aQlMNZjDhh\n",
      "\n",
      "#MWC20â€¦\n",
      "Business Process Analytics in R: Learn how to analyze business processes in R and extract actionable insights fromâ€¦ https://t.co/8R2XyvnG24\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'nSAgJsZv52Nr6FvXhuFOsYzUZ'\n",
    "consumer_secret = 'DopiaDunkGff9tJVlx5TZLH0F9xSBy3C4BqtudZgl83X6yJgvD'\n",
    "\n",
    "access_token = '588731047-ZNH0XOZimuwO76ELltyIV4ui1R3VaLSAjozUO9Qm'\n",
    "access_token_secret = '0FqKcvUWkmU3mL6Ya7SqmA7F2wf6cSQA4g1TBbG2W8YZE'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = api.statuses_lookup(['1234898286819061761'], tweet_mode=\"extended\") # id_list is the list of tweet ids\n",
    "df = pd.DataFrame()\n",
    "for i in tweets:\n",
    "    json = i._json\n",
    "    df = df.append({\n",
    "        'created_at': json['created_at'],\n",
    "        'full_text': json['full_text']\n",
    "    }, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Mar 03 17:47:49 +0000 2020</td>\n",
       "      <td>RT @drjawalsh: This is a provable lie. Public Health England have identified NO cases of #Covid_19 in Kettering where the hospital Johnsonâ€¦</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  \\\n",
       "0  Tue Mar 03 17:47:49 +0000 2020   \n",
       "\n",
       "                                                                                                                                     full_text  \n",
       "0  RT @drjawalsh: This is a provable lie. Public Health England have identified NO cases of #Covid_19 in Kettering where the hospital Johnsonâ€¦  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweet['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @drjawalsh: This is a provable lie. Public Health England have identified NO cases of #Covid_19 in Kettering where the hospital Johnsonâ€¦'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub(r'RT[\\s]+', '', text)\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "': This is a provable lie. Public Health England have identified NO cases of Covid_19 in Kettering where the hospital Johnsonâ€¦'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanText(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textBlob in c:\\users\\pharm\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from textBlob) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from nltk>=3.1->textBlob) (1.14.0)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\pharm\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from wordcloud) (7.0.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from wordcloud) (3.1.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from wordcloud) (1.18.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.14.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pharm\\anaconda3\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (46.1.3.post20200330)\n"
     ]
    }
   ],
   "source": [
    "!pip install textBlob\n",
    "from textblob import TextBlob\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06666666666666667\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def getPolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "print(getSubjectivity(text))\n",
    "print(getPolarity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.693396179108509, p_neg=0.3066038208914888)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "!python -m textblob.download_corpora\n",
    "TextBlob(text, analyzer=NaiveBayesAnalyzer()).sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\pharm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.128, 'neu': 0.872, 'pos': 0.0, 'compound': -0.4466}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "sid.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
